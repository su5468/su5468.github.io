---
layout: posts
categories:
    - Study
title: (전공복습) 인공지능 7. 신경망
---

# 차례

- [차례](#차례)
- [들어가기 전에](#들어가기-전에)
- [신경망(Neural Network)](#신경망neural-network)
  - [파라미터(Parameter)](#파라미터parameter)
  - [활성함수(Activation Function)](#활성함수activation-function)
    - [종류](#종류)
    - [Vanishing Gradient](#vanishing-gradient)
- [역전파(Backpropagation)](#역전파backpropagation)
  - [계산 그래프(Computational Graph)](#계산-그래프computational-graph)
  - [계산 그래프를 이용한 미분](#계산-그래프를-이용한-미분)
    - [연쇄법칙(Chain Rule)](#연쇄법칙chain-rule)
- [정리](#정리)

# 들어가기 전에
> 이 글은 컴퓨터학과 이중전공생으로서 배운 것들을 다시 한번 정리하고자 남기는 글입니다. 불완전한 기억 등의 이유로 오류가 있을 수 있으며, 참조 내지 이정표로만 사용해주세요.  
> 본 게시글은 고려대학교의 *인공지능* 강의를 기반으로 작성자의 추가적인 설명이 덧붙여진 글입니다. 또한 해당 강의는 UC 버클리 CS188을 기반으로 함을 밝힙니다.  

# 신경망(Neural Network)

신경망이란 무엇인가? 말 그대로 사람의 신경망과 *대략적으로* 유사한 부분이 있는 구조이다. 거칠게 말해서 선형함수(Linear Function)가 비선형 활성함수(Activation Function)로 연결되어 있는 것이 (인공)신경망이다.  
예를 들어, 선형함수 $$f = W_1x$$를 생각해보자. 이 함수는 하나의 입력 $$x$$와 그 입력에 대한 계수(가중치) $$W_0$$를 가진 매우 간단한 함수이다. $$g = \max(0, W_1x)$$는 비선형함수 $$\max(0, x)$$의 입력으로 $$f$$의 출력을 넣은 함수이다. 그리고 이 함수에 다시 선형함수가 연결되면 $$h = W_2 \max(0, W_1x)$$꼴이 될 것이다. 이는 선형함수 $$W_2x$$의 입력으로 $$g$$의 출력을 넣은 함수이다. 이러한 일을 원하는 만큼 반복하여 층을 쌓는 것이 바로 신경망이다.  

이처럼 신경망은 여러 개의 층으로 되어 있는데, 케이크의 단면을 생각하면 쉽게 이해할 수 있다. 케이크의 단면을 보면 시트(빵) 사이사이에 크림이 있다. 하나의 시트가 하나의 층(레이어)이 된다. 그리고 층 사이에는 활성함수(크림)가 존재한다.  
피처의 개수가 $$k$$라 할 때, 입력층(첫 번째 층)은 $$x \in \mathbb{R}^k$$라 할 수 있다. 즉, $$k$$개의 피처를 그대로 입력받는 것이다. 이때, $$k$$개의 피처를 $$k$$개의 값이라 볼 수도 있지만, $$k$$ 길이의 행렬(벡터)이라 볼 수도 있다. 즉, 입력층은 $$k$$개의 뉴런(노드)을 가지고, 길이 $$k$$의 벡터를 입력으로 받는 층이라는 것이다.  
그 다음 층(은닉층)은 임의의 개수의 뉴런을 갖게 되는데, 이를 $$l$$개라고 하면 $$h \in \mathbb{R}^l$$로 표현할 수 있다. 즉, 입력 레이어가 길이 $$k$$의 벡터를 입력으로 받아서, 길이 $$l$$의 벡터를 반환하고, 바로 다음의 은닉층이 그 벡터를 입력으로 받는 것이다. 따라서 입력 레이어에서 중간 레이어로 진행할 때의 가중치는 $$W_1 \in \mathbb{R}^{l \times k}$$라는 것을 알 수 있다. 즉, $$W_1$$은 $$l \times k$$ 행렬이라는 것이다.  
*(행렬곱의 필요조건을 생각해보면 이해할 수 있을 것이다.)*  
최종적으로 출력층 역시 마찬가지의 형태를 띄는데, $$y \in \mathbb{R}$$로 정의해보자. 이 경우 출력으로 1차원 실수 하나, 즉 실수 스칼라 하나를 결과값으로 얻게 된다. 이때 필요한 총 가중치의 개수는 입력층과 은닉층의 $$k \times l$$, 은닉층과 출력층의 $$l \times 1$$을 더해 총 $$kl + l$$이 된다. 물론 중간 레이어는 하나가 아니라 여러 개가 될 수 있으므로 $$kl_1 + l_1l_2 + l_2l_3 +...$$ 꼴이 될 것이다.  

## 파라미터(Parameter)

즉 신경망은 주어진 입력과 출력이 있을 때, 해당 입력과 출력의 관계를 가장 잘 표현하는 가중치들을 찾는 것이다. 이때, 각 선형함수의 값을 보정하기 위해 상수를 더할 수 있는데, 이 값을 편향(Bias) $$b$$이라 한다. 따라서 최종적으로 신경망은 아래와 같은 형태가 된다.  

1. 입력 $$x$$를 함수 $$W_1x + b_1$$에 넣어 계산한다.
2. 1번의 결과를 활성함수에 넣는다($$\max(0, W_1x + b_1)$$).
3. 2번의 결과를 또다른 함수에 넣는다($$W_2\max(0, W_1x + b_1) + b_2$$).
4. 3번의 결과를 활성함수에 넣는다($$\max(0, W_2\max(0, W_1x + b_1) + b_2)$$).
5. 이 과정을 원하는 횟수만큼 반복하여 층을 쌓는다.

이때, 입력은 데이터의 피처이므로 정해져 있다. 또한 우리가 바라는 출력, 즉 정답(Ground Truth) 역시 정해져 있다. 따라서 학습해야 하는 것은 각 선형함수의 가중치 $$W$$와 편향 $$b$$인데, 이처럼 학습해서 알아내야 하는 값을 파라미터(Parameter)라 한다.  
*(반대로, 학습해서 알아내지 않고 분석가가 임의로 정하는 값을 하이퍼 파라미터라 한다. 예를 들어 은닉층을 몇 개까지 쌓을지는 분석가가 결정할 문제다.)*  

## 활성함수(Activation Function)

신경망은 선형함수의 결과를 비선형함수에 넣고, 그 비선형함수의 결과를 다시 다른 선형함수의 입력으로 취하는 것을 반복하는 것인데, 이게 무슨 의미를 가질까? 특히, 비선형함수 $$\max(0, x)$$ 같은 건 왜 필요한 것일까?  
우선, 비선형함수가 없는, 선형함수만으로 이루어진 신경망을 생각해보자. $$f = W_0x$$이고, 비선형함수 없이 $$W_1x$$에 바로 $$f$$의 출력을 넣으면 그 결과는 $$h = W_1W_0x$$이다. 이걸 아무리 많이 반복해봐도 그 결과는 여전히 또다른 선형함수 $$...W_1W_0x = \overline{W}x$$이다. 이건 의미가 없는 짓이다. 왜냐하면 그냥 처음부터 $$\overline{W}$$에 해당하는 계수(가중치)를 찾으면 될 일이 아닌가? 여러 개의 가중치를 일일히 찾을 필요가 없을 것이다.  
즉, 선형함수는 몇 개를 합성하더라도 계속 선형함수이다. 따라서, 우리의 신경망이 충분히 복잡해지고, 비선형적인 문제를 풀 수 있게 하기 위해서 수식에 비선형함수를 더하는 것이다. 이때 이러한 비선형함수들을 활성함수(Activation Function)라 부른다. 앞서서 언급한 케이크의 비유를 들자면, 사이사이에 크림(활성함수)이 없으면 굳이 여러 개의 시트를 잘라서 넣을 필요가 없는 셈이다. 케이크의 맛이 더 풍요로워지기 위해서, 시트 사이에 크림을 넣어주는 것이다.  

### 종류

위에서는 활성함수의 예시로 $$\max(0, x)$$를 들었는데, 이는 ReLU라 불리는 고전적이고 유명한 활성함수이다. 사실 활성함수가 ReLU 하나만 있는 것은 아니고, 그 외에도 아래와 같은 함수들을 생각할 수 있다.  

![여러 활성함수들](https://github.com/su5468/su5468.github.io/assets/81351829/fd9edfe1-42f1-4746-a831-7c2ff9fa899d)  

- 시그모이드(Sigmoid): 모든 실수를 $$0$$부터 $$1$$ 사이에 위치시키는 S자 모양의 함수다.
  - 수식으로는 $$\sigma (x) = {1 \over 1 + e^{-x}}$$이다.
  - 이런 S자 형태의 함수를 전부 시그모이드라 부르기도 해서(예를 들어 아래의 Tanh), 이 경우 이 함수는 로지스틱 함수라고 한다.
- 하이퍼볼릭 탄젠트(Tanh): 마찬가지로 S자 형태를 지니는, 상기 로지스틱 함수와 유사한 형태의 함수로, 함숫값의 범위가 $$-1$$에서 $$1$$이다.
- ReLU: $$\max(0, x)$$로, 간단하게 $$0$$과 양수는 그대로 반환하고, 음수는 $$0$$으로 반환하는 함수이다.
  - 계산이 간편하고 양수 영역에서는 추후 설명할 Vanishing Graident 문제도 없다.
  - Dead ReLU: 반면 음수 지점에서는 기울기가 0이므로 노드의 활성화가 안 되는 문제가 발생한다.
- Leaky ReLU: Dead ReLU를 막기 위해, $$\max(0.1x, x)$$와 같이, 양수일 때는 $$x$$를 그대로 취하고 음수일 때는 아주 약하게 $$x$$를 반영하는 함수이다(물론 그 계수는 $$0.1$$외에 다양한 값으로 지정할 수 있다).
  - ReLU와 Leaky ReLU 모두 $$0$$에서 미분불가능하다(따라서 기울기를 구할 수 없다)는 문제를 지닌다.
  - 참고로, 음수일 때 $$x$$가 반영되는 정도(계수)를 직접 정하지 않고 파라미터로 두는 PReLU 역시 존재한다.
- ELU: ReLU와 Leaky ReLU가 $$0$$에서 기울기를 지니지 않는다는 문제를 해결하기 위해, 아래와 같은 공식을 사용하는 활성함수이다.
  - $$\begin{cases} x & x \ge 0 \\ \alpha (e^x -1) & x < 0\end{cases}$$
  - 즉, $$x$$가 음수이면 기울기가 $$0$$에 수렴하도록 점차 줄어들게 되는데, $$0$$에서 미분가능하기 때문에 문제를 해결할 수 있다.
- Maxout: 구간별로 여러 선형함수의 최댓값(Max)을 찾아서 사용하는 방식이다.
  - 여러 개의 입력을 받아서, 각 구간별로 선형함수의 최댓값을 취한다.
  - 예를 들어 두 개의 입력을 받는다면 $$\max(w_1^T x + b_1, w_2^T x + b_2)$$로 나타낼 수 있다.
  - 사실 Maxout은 ReLU의 일반화로도 볼 수 있는데, 두 개의 입력을 받는 Maxout에서 하나의 입력을 $$0$$으로 놓으면 ReLU가 되기 때문이다.
- Softmax: 시그모이드(로지스틱) 함수와 유사하게, 모든 실수를 $$0$$에서 $$1$$로 변환한다.
  - 수식으로는 $${e^{x_i} \over \sum_{j=1}^d e^{x_j}}$$로 표기한다.
  - 모든 Softmax 출력의 총합은 $$1$$이라는 성질을 가진다. 즉, Softmax는 여러가지 케이스가 있는 상황에서 각 케이스의 확률을 나타내는 데에 사용할 수 있다.

### Vanishing Gradient

몇몇 활성함수는 Vanishing Gradient 문제를 겪기도 한다. Vanishing Gradient란 우리말로 하자면 기울기가 증발 내지는 소실한다는 뜻인데, 시그모이드 함수처럼 미분 결과(기울기)가 너무 작은 경우 계산 과정에서 미분 결과가 계속 곱해지다가 너무 작아서 소실되어버리는 문제를 이야기한다. 그 결과 기울기가 사실상 $$0$$이 되어버리고, 학습이 되지 않거나 매우 조금 이루어지는 현상이 나타난다.  
*(미분 결과들을 왜 곱하느냐? 그 이야기는 바로 밑에서 다룰 역전파 항목을 참조하라.)*  

# 역전파(Backpropagation)

신경망 모델의 계산은 위에서 언급한 것처럼 입력층에 피처들을 넣고, 그 결과에 활성함수를 적용하여 은닉층에 넣고, 다시 그 은닉층의 결과에 활성함수를 적용하여 다음 은닉층에 넣고, 이를 원하는 만큼 반복한 후 출력층에 내보내는 형태이다. 이러한 과정을 순전파(Forward Propagation, Feed Forward)라고 한다. 계산이 입력층에서 출력층으로, 즉 정방향으로 이루어졌다는 의미이다.  
이렇게 정방향으로 계산한 결과 출력과 실제 우리가 바라는 정답(Ground Truth)을 비교하면 각 레이어(층)의 파라미터가 잘 설정되었는지 알 수 있다. 이때, 정답과 출력 사이의 오차를 계산하기 위해 사용하는 것이 바로 손실함수이다. 이제 출력이 정답에 가깝도록 파라미터를 업데이트해야 하는데, 어떤 값으로 파라미터를 바꾸어야 하는지 어떻게 알 수 있을까?  

그 방법이 바로 역전파(Backpropagation)이다. 순전파는 입력층에서 출력층으로 값을 계산해서 넘겨주는 과정이었는데, 역전파는 반대로 출력층에서 입력층 쪽으로 값을 계산하며 파라미터를 업데이트한다.  
우리는 어떤 방향으로 파라미터를 업데이트해야 하는가? 바로 오차(손실함수의 결과값)가 적어지는 방향일 것이다. 손실함수의 값을 줄여야 하기 때문에, 미분을 할 필요가 있다. 미분 결과는 기울기를 의미하므로 파라미터가 어느 방향으로 얼마나 이동해야 값이 작아지는지 알 수 있으며, $$0$$이 되는 지점이 극소라는 것을 알 수 있다.  

## 계산 그래프(Computational Graph)

역전파를 쉽게 이해하기 위해 계산 그래프를 알아보자. 계산 그래프란 계산 과정을 그래프의 형태로 나타낸 것을 의미한다.  
![계산 그래프 예제1](https://github.com/su5468/su5468.github.io/assets/81351829/d30942bd-834d-4425-a4a0-b5f6df0467b3)  
예를 들어 위 계산 그래프는 $$(x + y)z$$를 의미한다.  
![계산 그래프 예제2](https://github.com/su5468/su5468.github.io/assets/81351829/b7b76d85-33aa-41a2-873a-a4e1d13dcf0a)  
입력으로 $$-2, 5, -4$$를 넣으면 $$(-2 + 5) \times -4 = -12$$가 계산 결과가 될 것이다. 이러한 과정은 계산 그래프 상의 순전파에 해당한다.  

## 계산 그래프를 이용한 미분

계산 그래프를 이용하면 미분 계산을 쉽게 할 수 있다. 예를 들어, 위 상황에서 각 입력의 값이 변할 때마다 결과값은 얼마나 변하는가? 단적으로 생각해서 단위 $$x$$에 대한 $${\partial \over \partial x} (x + 5) \times -4 = -4$$이다. 마찬가지로 $${\partial \over \partial y} (-2 + y) \times -4 = -4$$이고, $${\partial \over \partial z} (-2 + 5) \times z = 3$$이다.  
지금은 식이 간단하여 단순히 계산할 수 있지만, 복잡한 식에 대하여 각 입력에 대한 편미분을 취하는 것은 어려운 일이다. 따라서 계산 그래프를 이용해서 각 입력에 대해 결과를 미분해보자.  

1. 우선은 $${\partial f \over \partial f} = 1$$에서 시작해보자.
2. 그럼 $$q = x + y$$에 대해 $${\partial f \over \partial q} = {\partial \over \partial q} -4q = -4$$라는 것을 알 수 있다.
3. 또한 $${\partial q \over \partial x} = {\partial \over \partial x} x + y = 1$$이라는 것도 알 수 있다.
4. 추후 설명할 연쇄법칙(Chain Rule)에 의해, $${\partial f \over \partial x} = {\partial f \over \partial q} {\partial q \over \partial x} = -4 \times 1 = -4$$로 계산할 수 있다.
5. 즉, 각 입력에 대해 결과 $$f$$를 미분하기 위해서, $$f$$에서 시작하여 입력 쪽으로 미분하고 그 결과를 곱하면 되는 것이다.
6. 이 과정을 모든 연결된 노드에 대해 반복하면 각 입력에 대한 기울기를 구할 수 있다.

![계산 그래프 예제3](https://github.com/su5468/su5468.github.io/assets/81351829/adff59a8-e2a0-4d78-b90b-e49bbd190a3f)  
위 그림은 그렇게 구한 각 입력에 따른 $$f$$의 미분 결과이다.  

### 연쇄법칙(Chain Rule)

중간의 연쇄법칙이란, 합성함수의 미분을 위해 사용하는 법칙으로, 아래 법칙을 의미한다.  

- $${\partial f \over \partial x} = {\partial f \over \partial z}{\partial z \over \partial x}$$

설명하자면, $$x$$에 대해 $$f$$를 미분하려면, 다른 변수 $$z$$에 대해 $$f$$를 미분하고 $$x$$에 대해 $$z$$를 미분한 것을 곱해도 된다는 것이다. 당연히 이 과정은 몇번이든 반복할 수 있다. 예를 들어 아래와 같이 말이다.  

- $${\partial z \over \partial a} = {\partial b \over \partial a} {\partial c \over \partial b} ... {\partial y \over \partial x} {\partial z \over \partial y}$$

이제 연쇄법칙을 통해 역전파를 이야기하면, Local Gradient와 Upstream Gradient를 곱해서, Downstream Gradient를 구하는 과정이라 할 수 있다. 

- Local Gradient: 현재 노드의 수식의 미분값을 의미한다.
  - 예를 들어, 현재 노드가 덧셈 노드 $$q = x + y$$라면, $$x$$에 대한 Local Gradient는 $$1$$일 것이다.
- Upstream Gradient: 이전 노드(순전파 방향으로 보았을 때는 다음 노드)의 역전파 결과를 의미한다.
  - 예를들어, 위 계산 그래프 예제처럼 덧셈 노드 다음에 곱셈 노드 $$f = q \times z$$가 등장한다면 $$q$$에 대한 미분 결과는 $$z$$이고, $$q = x + y$$ 입장에서 Upstream Gradient는 $$z$$이다.
- Downstream Gradient: 위의 두 미분 결과를 곱한 $$1 \times z = z$$이다. 
  - 이제 이 Downstream Gradient가 $$q = x + y$$의 역전파 결과가 되고, 동시에 다음 노드(순전파 방향으로는 이전 노드)의 Upstream Gradient가 된다.

# 정리

이 게시글에서는 [신경망](#신경망neural-network)과 [역전파](#역전파backpropagation)에 대해 알아보았다. 중요한 내용이 많았으며, 이러한 신경망 관련 내용은 딥러닝에 꼭 필요한 부분이니 알고 있어야 할 것이다.  

이렇게 인공지능 강의 복습이 모두 끝났다. 다음 강의는 무엇으로 할까 고민 중이다.  
